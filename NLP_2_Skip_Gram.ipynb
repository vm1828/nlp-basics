{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# SETUP"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Constants"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generating skip-grams\n",
                "\n",
                "CONTEXT_WINDOW_SIZE = 12\n",
                "\n",
                "# Hyperparameters\n",
                "EPOCHS = 100\n",
                "LR = 0.001\n",
                "BATCH_SIZE = 32\n",
                "EMB_DIM = 30"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Import"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from itertools import chain\n",
                "import re\n",
                "\n",
                "\n",
                "from shared.constants import *\n",
                "from shared.corpus import Corpus\n",
                "from shared.utils import remove_punctuation\n",
                "from shared.models import SavedModelSkipGram, HyperparametersSkipGram\n",
                "\n",
                "\n",
                "from nltk import word_tokenize, sent_tokenize\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "from torch.optim import Adam\n",
                "from torch.utils.data import TensorDataset, DataLoader\n",
                "import lightning as L\n",
                "from pytorch_lightning.loggers import TensorBoardLogger\n",
                "\n",
                "from sklearn.manifold import TSNE\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# DATA"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly norma\n",
                        "6234344\n"
                    ]
                }
            ],
            "source": [
                "raw_text = Corpus(DIR_HP).text\n",
                "print(raw_text[:100])\n",
                "print(len(raw_text))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Tokenized Corpus"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "def tokenize_sentences(text: str) -> list[str]:\n",
                "    sentences = sent_tokenize(text)\n",
                "    tokenized_corpus = []\n",
                "\n",
                "    for sentence in sentences:\n",
                "        sentence = remove_punctuation(sentence)\n",
                "        tokens = []\n",
                "        for word in word_tokenize(sentence):\n",
                "            if word:\n",
                "                tokens.append(word)\n",
                "        tokens.append('<EOS>')\n",
                "        tokenized_corpus.append(tokens)\n",
                "    return tokenized_corpus"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "62944\n"
                    ]
                }
            ],
            "source": [
                "# tokenize text\n",
                "tokenized_corpus = tokenize_sentences(raw_text)\n",
                "print(len(tokenized_corpus))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Indexed Corpus"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[4214, 7951, 4217, 1902, 20863, 20723, 15418, 5050, 1848, 30084, 22733, 28157, 24571, 27825, 27869, 30084, 21718, 20638, 27816, 30694, 29589, 20218, 79]\n",
                        "62944\n",
                        "30738\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "# Build vocabulary\n",
                "vocab = sorted(set(chain.from_iterable(tokenized_corpus))) # Sorting for deterministic indexing (=reproducibility)\n",
                "WORD2IDX = {word: idx for idx, word in enumerate(vocab)}\n",
                "IDX2WORD = {idx: word for word, idx in enumerate(vocab)}\n",
                "# WORD2IDX = {word: idx for idx, word in enumerate(set(sum(tokenized_corpus, [])))}\n",
                "# IDX2WORD = {idx: word for word, idx in WORD2IDX.items()}\n",
                "VOCAB_SIZE = len(vocab)\n",
                "\n",
                "# Convert tokenized corpus to indexed corpus\n",
                "indexed_corpus = [[WORD2IDX[word] for word in sentence] for sentence in tokenized_corpus]\n",
                "\n",
                "print(indexed_corpus[0])\n",
                "print(len(indexed_corpus))\n",
                "print(VOCAB_SIZE)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "True"
                        ]
                    },
                    "execution_count": 7,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "'Harry' in vocab"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Skip-Gram Pairs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "(4214, 7951)\n",
                        "100\n"
                    ]
                }
            ],
            "source": [
                "def generate_skipgrams(indexed_corpus, window_size=CONTEXT_WINDOW_SIZE): \n",
                "    data = []\n",
                "    for sentence in indexed_corpus:\n",
                "        for i, target in enumerate(sentence):\n",
                "            window = range(max(0, i-window_size), min(len(sentence), i+window_size+1))\n",
                "            context = [sentence[j] for j in window if j!=i]\n",
                "            for ctx in context:\n",
                "                data.append((target, ctx)) # pair every word with each word in the context\n",
                "    return data\n",
                "\n",
                "skipgram_pairs = generate_skipgrams(indexed_corpus)[:100] # NOTE: Only 100 pairs for test run !!!\n",
                "print(skipgram_pairs[0])\n",
                "print(len(skipgram_pairs))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Train Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_train = torch.LongTensor([pair[0] for pair in skipgram_pairs])  # target words\n",
                "y_train = torch.LongTensor([pair[1] for pair in skipgram_pairs])  # context words\n",
                "# nn.Embedding and nn.CrossEntropyLoss require torch.int64 (LongTensor) inputs for indexing and categorical targets.\n",
                "\n",
                "dataset = TensorDataset(X_train, y_train)\n",
                "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# MODEL"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "## From Scratch\n",
                "# class SkipGramModelFromScratch(L.LightningModule):\n",
                "\n",
                "#     def __init__(self):\n",
                "#         super().__init__()\n",
                "#         L.seed_everything(seed=42)\n",
                "#         self.input_to_hidden = nn.Linear(in_features=len(dataset), out_features=EMB_DIM, bias=False)\n",
                "#         self.hidden_to_output = nn.Linear(in_features=2, out_features=4, bias=False)\n",
                "#         self.loss = nn.CrossEntropyLoss()\n",
                "\n",
                "#     def forward(self, input):\n",
                "#         hidden = self.input_to_hidden(input)\n",
                "#         # Then we pass \"hidden\" to the weights we created with nn.Linear() between the hidden layer and the output.\n",
                "#         output_values = self.hidden_to_output(hidden)\n",
                "#         return(output_values)\n",
                "\n",
                "\n",
                "#     def configure_optimizers(self):\n",
                "#         return Adam(self.parameters(), lr=LR)\n",
                "\n",
                "\n",
                "#     def training_step(self, batch, batch_idx):\n",
                "#         input_i, label_i = batch\n",
                "#         output_i = self.forward(input_i)\n",
                "#         loss = self.loss(output_i, label_i)\n",
                "        \n",
                "#         self.log(\"train_loss\", loss)\n",
                "\n",
                "#         return loss"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## nn.Embedding"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
                        "GPU available: False, used: False\n",
                        "TPU available: False, using: 0 TPU cores\n",
                        "HPU available: False, using: 0 HPUs\n",
                        "\n",
                        "  | Name       | Type             | Params | Mode \n",
                        "--------------------------------------------------------\n",
                        "0 | embeddings | Embedding        | 922 K  | train\n",
                        "1 | linear     | Linear           | 952 K  | train\n",
                        "2 | loss       | CrossEntropyLoss | 0      | train\n",
                        "--------------------------------------------------------\n",
                        "1.9 M     Trainable params\n",
                        "0         Non-trainable params\n",
                        "1.9 M     Total params\n",
                        "7.500     Total estimated model params size (MB)\n",
                        "3         Modules in train mode\n",
                        "0         Modules in eval mode\n",
                        "/home/asdf/Desktop/nlp-basics/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
                        "/home/asdf/Desktop/nlp-basics/.venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 99: 100%|██████████| 4/4 [00:00<00:00, 23.46it/s, v_num=0]"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 99: 100%|██████████| 4/4 [00:00<00:00, 13.90it/s, v_num=0]\n",
                        "Model saved to models/skipgram_context12_emb30_e100_lr0.001.pth\n"
                    ]
                }
            ],
            "source": [
                "# %%capture\n",
                "# sys.exit(\"Manual execution required\")\n",
                "\n",
                "MODEL_NAME = f'skipgram_context{CONTEXT_WINDOW_SIZE}_emb{EMB_DIM}_e{EPOCHS}_lr{LR}'\n",
                "HYPERPARAMETERS = HyperparametersSkipGram(EPOCHS, LR, BATCH_SIZE, EMB_DIM, VOCAB_SIZE)\n",
                "\n",
                "class SkipGramModelEmbedding(L.LightningModule):\n",
                "\n",
                "    def __init__(self, vocab_size, emb_dim):\n",
                "        super().__init__()\n",
                "        self.embeddings = nn.Embedding(vocab_size, emb_dim)\n",
                "        self.linear = nn.Linear(emb_dim, vocab_size)\n",
                "        self.loss = nn.CrossEntropyLoss()\n",
                "\n",
                "    def forward(self, context_word):\n",
                "        embedding = self.embeddings(context_word)\n",
                "        output = self.linear(embedding)\n",
                "        return output\n",
                "    \n",
                "    def configure_optimizers(self):\n",
                "        return Adam(self.parameters(), lr=LR)\n",
                "\n",
                "    def training_step(self, batch, batch_idx):\n",
                "        input_i, label_i = batch\n",
                "        output_i = self.forward(input_i)\n",
                "        loss = self.loss(output_i, label_i)\n",
                "\n",
                "        self.log(\"train_loss\", loss)\n",
                "        \n",
                "        return loss\n",
                "    \n",
                "    def on_train_end(self):\n",
                "        \"\"\"Save the model after training is complete.\"\"\"\n",
                "        saved_model = SavedModelSkipGram(\n",
                "            model_state_dict=self.state_dict(),\n",
                "            model_hyperparameters=HYPERPARAMETERS\n",
                "        )\n",
                "        saved_model.save_model(MODEL_NAME)\n",
                "\n",
                "if os.path.exists(os.path.join('models', f'{MODEL_NAME}.')):\n",
                "    raise FileExistsError(f\"Model already exists, please choose a different model name.pth\")\n",
                "\n",
                "modelEmbedding = SkipGramModelEmbedding(VOCAB_SIZE, EMB_DIM)\n",
                "trainer = L.Trainer(max_epochs=EPOCHS, logger=TensorBoardLogger(f\"models/logs/\", name=MODEL_NAME))\n",
                "trainer.fit(modelEmbedding, train_dataloaders=dataloader)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# DEMO"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "<All keys matched successfully>"
                        ]
                    },
                    "execution_count": 12,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Load model\n",
                "\n",
                "checkpoint = torch.load(f\"models/{MODEL_NAME}.pth\", weights_only=False)\n",
                "\n",
                "hyperparameters: HyperparametersSkipGram = checkpoint.model_hyperparameters\n",
                "model = SkipGramModelEmbedding(hyperparameters.vocab_size, hyperparameters.emb_dim)\n",
                "model.load_state_dict(checkpoint.model_state_dict)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": [
                "def visualize_word_embeddings(model):\n",
                "  \n",
                "  words = [\n",
                "    'Harry', 'Hermione', 'Ron', 'Dumbledore', 'Snape', 'Hagrid', 'Voldemort',\n",
                "    'Draco', 'Neville', 'McGonagall', 'Quirrell', 'Dursley', 'Hedwig',\n",
                "    'Gryffindor', 'Slytherin', 'Hogwarts', 'Quidditch', 'wand', 'potion',\n",
                "    'spell', 'broomstick', 'owl', 'wizard', 'Muggle', 'witch', 'stone'\n",
                "  ]\n",
                "  \n",
                "  indices = [WORD2IDX[word] for word in words]\n",
                "  embeddings = model.embeddings.weight.data[indices]\n",
                "\n",
                "  tsne = TSNE(n_components=2, perplexity=20, random_state=42) # 2d t-SNE\n",
                "  embeddings_2d = tsne.fit_transform(embeddings)\n",
                "\n",
                "  colors = sns.husl_palette(n_colors = len(words))\n",
                "  plt.figure(figsize=(10, 10))\n",
                "  for i, word in enumerate(words):\n",
                "    plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1],c=colors)\n",
                "    plt.annotate(word, xy=(embeddings_2d[i, 0], embeddings_2d[i, 1]))\n",
                "  \n",
                "  plt.show();"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "visualize_word_embeddings(model)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
