{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "40ce84c3",
            "metadata": {},
            "source": [
                "# SETUP"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "093fa485",
            "metadata": {},
            "source": [
                "## Import"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "7707d486",
            "metadata": {},
            "outputs": [],
            "source": [
                "import re\n",
                "import json\n",
                "# import requests\n",
                "# from bs4 import BeautifulSoup\n",
                "\n",
                "from shared.constants import *\n",
                "from shared.corpus import Corpus\n",
                "from shared.utils import load_json\n",
                "\n",
                "import spacy\n",
                "from spacy.language import Language\n",
                "from spacy.lang.en import English"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "0b16abe4",
            "metadata": {},
            "outputs": [],
            "source": [
                "# # Get characters\n",
                "\n",
                "# url = \"https://pemberley.com/janeinfo/ppdrmtis.html\"\n",
                "# res = requests.get(url)\n",
                "# soup = BeautifulSoup(res.content, \"html.parser\")\n",
                "\n",
                "# character_names = []\n",
                "# target_h2 = soup.find(\"h2\", string=\"Brief, Organized Listing of Characters\")\n",
                "# target_ul = target_h2.find_next(\"ul\")\n",
                "# for a in target_ul.find_all(\"a\"):\n",
                "#     name = a.get_text(strip=True).replace('\\n', ' ')\n",
                "#     if name:\n",
                "#         character_names.append(name)\n",
                "\n",
                "# print(character_names)\n",
                "# with open(JSON_CHARACTERS_PNP, \"w\", encoding=\"utf-8\") as f:\n",
                "#     json.dump(character_names, f, indent=4)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8664b4e3",
            "metadata": {},
            "source": [
                "# Rule-Based NER (Gazetteer Method)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "f558c621",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "['Mr. Bennet', 'Mrs. Bennet', 'Jane', 'Elizabeth', 'Mary', 'Kitty', 'Lydia', 'Bingley', 'Louisa Hurst', 'Caroline', 'Mr. Collins', 'Old Mr. Darcy', 'Lady Anne Darcy', 'Darcy', 'Georgiana Darcy', 'Lady Catherine', 'Anne de Bourgh', 'Colonel Fitzwilliam', 'Mr. Gardiner', 'Mrs. Gardiner', 'Sir William', 'Lady Lucas', 'Charlotte', 'Maria', 'Old Mr. Wickham', 'Wickham', 'Mrs. Annesley', 'Captain Carter', 'Mr. Chamberlayne', 'Dawson', 'Mr. Denny', 'Colonel Forster', 'William Goulding', 'Miss Grantley', 'Haggerston', 'The Harringtons', 'Mrs. Hill', 'Mr. Hurst', 'Mrs. Jenkinson', 'Mr. Jones', 'Miss Mary King', 'Mrs. Long', 'Lady Metcalfe', 'Mr. Morris', 'Mrs. Nicholls', 'Mr. Philips', 'Miss Pope', 'Mr. Pratt', 'Mrs. Reynolds', 'Mr. Robinson', 'Mr. Stone', 'Miss Watson', 'The Miss Webbs', 'Mrs. Younge']\n",
                        "['Anne', 'Annesley', 'Bennet', 'Bingley', 'Bourgh', 'Caroline', 'Carter', 'Catherine', 'Chamberlayne', 'Charlotte', 'Collins', 'Darcy', 'Dawson', 'Denny', 'Elizabeth', 'Fitzwilliam', 'Forster', 'Gardiner', 'Georgiana', 'Goulding', 'Grantley', 'Haggerston', 'Harringtons', 'Hill', 'Hurst', 'Jane', 'Jenkinson', 'Jones', 'King', 'Kitty', 'Long', 'Louisa', 'Lucas', 'Lydia', 'Maria', 'Mary', 'Metcalfe', 'Morris', 'Nicholls', 'Philips', 'Pope', 'Pratt', 'Reynolds', 'Robinson', 'Stone', 'Watson', 'Webbs', 'Wickham', 'William', 'Younge']\n"
                    ]
                }
            ],
            "source": [
                "# Load data\n",
                "with open(TXT_PNP, \"r\", encoding=\"utf-8\") as f:\n",
                "    text = f.read().strip()\n",
                "    text = re.split(PATTERN_CHAPTER_LINE, text)[1:6]\n",
                "\n",
                "with open(JSON_CHARACTERS_PNP, \"r\", encoding=\"utf-8\") as f:\n",
                "    characters = json.load(f)\n",
                "    names = set()\n",
                "    for character in characters:\n",
                "        names.update(\n",
                "            [\n",
                "                i for i in character.split() \n",
                "                if not i in ['Mr.', 'The', 'Mrs.', 'Miss', 'Old', 'Lady', 'de', 'Sir', 'Captain', 'Colonel']\n",
                "            ]\n",
                "        )\n",
                "    names = sorted(names)\n",
                "\n",
                "print(characters)\n",
                "print(names)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "c5a3dade",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "\n",
                        "Chapter 1:\n",
                        "Bingley, Jane, Lady Lucas, Lydia, Mr Bennet, Mr Bingley, Mr Morris, Mrs Long, Sir William\n",
                        "\n",
                        "\n",
                        "Chapter 2:\n",
                        "Elizabeth, Kitty, Lydia, Mary, Mr Bennet, Mr Bingley, Mrs Bennet, Mrs Long, Now Kitty, While Mary\n",
                        "\n",
                        "\n",
                        "Chapter 3:\n",
                        "Bingley, Catherine, Come Darcy, Elizabeth, Elizabeth Bennet, Jane, Lady Lucas, Lydia, Maria, Maria Lucas, Mary, Miss Bennet, Miss Bingley, Miss King, Miss Lucas, Mr Bennet, Mr Bingley, Mr Darcy, Mr Hurst, Mrs Bennet, Mrs Hurst, Sir William\n",
                        "\n",
                        "\n",
                        "Chapter 4:\n",
                        "Bingley, Darcy, Elizabeth, Miss Bennet, Miss Bingley, Mr Bingley, Mrs Hurst, When Jane\n",
                        "\n",
                        "\n",
                        "Chapter 5:\n",
                        "Charlotte, Elizabeth, Jane, Lady Lucas, Lucas, Mary, Miss Bennet, Miss Bingley, Miss Lucas, Mr Darcy, Mr Robinson, Mrs Bennet, Mrs Long, Sir William, William Lucas\n"
                    ]
                }
            ],
            "source": [
                "# Characters by Chapter\n",
                "for n_chapter, chapter in enumerate(text, start=1):\n",
                "    print(f'\\n\\nChapter {n_chapter}:')\n",
                "    chapter = chapter.strip().replace('\\n', ' ').translate(str.maketrans('', '', PUNCTUATION))\n",
                "    words = chapter.split() # NOTE: Mr and Mrs will be split\n",
                "\n",
                "    chapter_names = set()\n",
                "    for n, word in enumerate(words):\n",
                "        if word in names:\n",
                "            if (words[n-2] == 'Old') or (words[n-1] == 'de'):\n",
                "                chapter_names.add(' '.join(words[n-2:n+1])) # Handle 3-word names\n",
                "            elif words[n-1][0].isupper():\n",
                "                chapter_names.add(' '.join(words[n-1:n+1])) # Handle 2-word names\n",
                "            else:\n",
                "                chapter_names.add(word)\n",
                "    chapter_names = sorted(chapter_names)\n",
                "    print(', '.join(chapter_names))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8b7c17a2",
            "metadata": {},
            "source": [
                "# NER with SpaCy"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "353dffb6",
            "metadata": {},
            "source": [
                "## Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "e8aa811c",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly norma'"
                        ]
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Data\n",
                "hp_corpus = Corpus(DIR_HP)\n",
                "chapter1 = hp_corpus.books[0].chapters[0].text\n",
                "chapter1[:100]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "c83056b6",
            "metadata": {},
            "outputs": [],
            "source": [
                "# url = \"https://en.wikipedia.org/wiki/List_of_Harry_Potter_characters\"\n",
                "# res = requests.get(url)\n",
                "# soup = BeautifulSoup(res.content, \"html.parser\")\n",
                "\n",
                "# characters = []\n",
                "# divs = soup.find_all(\"div\", class_=\"mw-heading mw-heading3\")\n",
                "# for div in divs:\n",
                "#     ul = div.find_next(\"ul\")\n",
                "#     for li in ul.find_all(\"li\"):\n",
                "#             text = li.get_text(strip=True)\n",
                "#             name = re.search(r\"^(.*?)–\", text).group(1).replace(' and ', ',').translate(str.maketrans('()', ',,'))\n",
                "#             if ',' in name:\n",
                "#                 split_name = [part.strip() for part in name.split(',') if part.strip()]\n",
                "#                 last_name = split_name[0]\n",
                "\n",
                "#                 # Case: <last_name>, <name 1>, <name2>, ...\n",
                "#                 if len(split_name[1:]) > 1:\n",
                "#                     for first_name in split_name[1:]:\n",
                "#                         characters.append(f'{first_name.strip()} {last_name.strip()}')\n",
                "#                 # Case: <last_name>, <first_name>\n",
                "#                 else:\n",
                "#                     characters.append(f'{split_name[1].strip()} {last_name.strip()}')\n",
                "#             else:\n",
                "#                 # Case: <name>\n",
                "#                 characters.append(name.strip())\n",
                "\n",
                "# print(characters[:10])\n",
                "# with open(JSON_CHARACTERS_HP, \"w\", encoding=\"utf-8\") as f:\n",
                "#     json.dump(characters, f, indent=4)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "0edc4c26",
            "metadata": {},
            "outputs": [],
            "source": [
                "nlp = spacy.load('en_core_web_lg')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "2f3503be",
            "metadata": {},
            "outputs": [],
            "source": [
                "doc = nlp(chapter1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "1f2cf7b0",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Dursley        PERSON    \n",
                        "number four    CARDINAL  \n",
                        "Privet Drive   PERSON    \n",
                        "Grunnings      ORG       \n",
                        "Dursleys       NORP      \n",
                        "Dudley         PERSON    \n",
                        "Potters        ORG       \n",
                        "Potter         PERSON    \n",
                        "several years  DATE      \n",
                        "Tuesday        DATE      \n"
                    ]
                }
            ],
            "source": [
                "unique_ents = list({ent.text: ent for ent in doc.ents}.values())\n",
                "for ent in unique_ents[:10]:\n",
                "    print(f'{ent.text:<15}{ent.label_:<10}')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5fc7a6f8",
            "metadata": {},
            "source": [
                "We need to customize the model to make NER for the HP corpus better"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "9d337ecf",
            "metadata": {},
            "source": [
                "# MODELS"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "12155170",
            "metadata": {},
            "source": [
                "## Rule-Based Model (with EntityRuler)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b37b446b",
            "metadata": {},
            "source": [
                "Using EntityRuler pipe we can generate rule-based NER with SpaCy.  \n",
                "This approach can be used to generate train datasets."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "6e61a369",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "4360\n",
                        "{'label': 'PERSON', 'pattern': '\"Delphi\"'}\n",
                        "{'label': 'PERSON', 'pattern': 'Abbott'}\n",
                        "{'label': 'PERSON', 'pattern': 'Aberforth'}\n",
                        "{'label': 'PERSON', 'pattern': 'Aberforth Dumbledore'}\n",
                        "{'label': 'PERSON', 'pattern': 'Alastor'}\n"
                    ]
                }
            ],
            "source": [
                "# Generate patterns for characters\n",
                "\n",
                "def generate_characters(characters: list[str]) -> list:\n",
                "    \"\"\"Generate combinations of character names and titles\"\"\"\n",
                "    names = []\n",
                "    titles = [\"Dr.\", \"Professor\", \"Mr.\", \"Mrs.\", \"Ms.\", \"Miss\", \"Aunt\", \"Uncle\", \"Mr. and Mrs.\"]\n",
                "    \n",
                "    for item in characters:\n",
                "        item = item.replace('The', '').replace('the', '').strip()\n",
                "        names.append(item)\n",
                "        for name in item.split():\n",
                "            names.append(name)\n",
                "    \n",
                "    generated_characters = set()\n",
                "    for title in titles:\n",
                "        for name in names:\n",
                "            generated_characters.add(name)\n",
                "            generated_characters.add(f'{title} {name}')\n",
                "    generated_characters = sorted(generated_characters)\n",
                "    return generated_characters\n",
                "\n",
                "def generate_characters_patterns(generated_characters: list) -> list[dict]:\n",
                "    patterns = []\n",
                "    for item in generated_characters:\n",
                "        pattern = {\n",
                "            \"label\": \"PERSON\",\n",
                "            \"pattern\": item\n",
                "        }\n",
                "        patterns.append(pattern)\n",
                "    return (patterns)\n",
                "\n",
                "data = load_json(JSON_CHARACTERS_HP)\n",
                "generated_characters = generate_characters(data)\n",
                "patterns = generate_characters_patterns(generated_characters)\n",
                "print(len(patterns))\n",
                "for pattern in patterns[:5]:\n",
                "    print(pattern)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "9a2345a7",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create rule-based NER model with EntityRuler pipe\n",
                "\n",
                "path_hp_rule_based_ner_model = os.path.join(DIR_MODELS_SPACY, \"hp_rule_based_ner\")\n",
                "\n",
                "def create_rule_based_model(\n",
                "        patterns: list[dict], \n",
                "        path=path_hp_rule_based_ner_model\n",
                "    ) -> None:\n",
                "    \"\"\"Create and save a rule-based NER model using custom patterns.\"\"\"\n",
                "\n",
                "    # Initialize model with an empty pipeline\n",
                "    nlp = English() # no pre-trained components (only basic tokenizer included)\n",
                "\n",
                "    # Add EntityRuler to the model\n",
                "    ruler = nlp.add_pipe(\"entity_ruler\")\n",
                "    ruler.add_patterns(patterns)\n",
                "\n",
                "    # Save model\n",
                "    nlp.to_disk(path)\n",
                "\n",
                "def test_rule_based_model(nlp: Language, text: str) -> list[str]:\n",
                "    \"\"\"Extract recognized entities.\"\"\"\n",
                "    doc = nlp(text)\n",
                "    return sorted(set(ent.text for ent in doc.ents))\n",
                "\n",
                "create_rule_based_model(patterns)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "4443cbbe",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Book 1, Chapter 1\n",
                        "['Albus', 'Albus Dumbledore', 'Dedalus Diggle', 'Dudley', 'Dumbledore', 'Godric', 'Hagrid', 'Harry', 'Harry Potter', 'James', 'James Potter', 'Lily', 'Madam', 'Mr. Dursley', 'Mr. and Mrs. Dursley', 'Mrs. Dursley', 'Mrs. Potter', 'Muggle', 'Petunia', 'Pomfrey', 'Potter', 'Professor Dumbledore', 'Professor McGonagall', 'Sirius', 'Sirius Black', 'Ted', 'Voldemort']\n",
                        "\n",
                        "Book 1, Chapter 2\n",
                        "['Aunt Petunia', 'Dudley', 'Dudley Dursley', 'Harry', 'Harry Potter', 'Marge', 'Mr. Dursley', 'Mrs. Figg', 'Nearly', 'Uncle Vernon', 'Vernon']\n",
                        "\n",
                        "Book 1, Chapter 3\n",
                        "['Aunt Petunia', 'Dudley', 'Harry', 'Marge', 'Mrs. Figg', 'Nearly', 'Petunia', 'Potter', 'Uncle Vernon', 'Vernon']\n"
                    ]
                }
            ],
            "source": [
                "# Test model\n",
                "\n",
                "hp_corpus = Corpus(DIR_HP)\n",
                "nlp = spacy.load(path_hp_rule_based_ner_model)\n",
                "\n",
                "for n_book, book in enumerate(hp_corpus.books[:1], start=1):\n",
                "    for n_chapter, chapter in enumerate(book.chapters[:3], start=1): # NOTE: only book 1 chapters 1-3\n",
                "        print(f'\\nBook {n_book}, Chapter {n_chapter}')\n",
                "        results = test_rule_based_model(nlp, chapter.text)\n",
                "        results = sorted(set(results))\n",
                "        print(results)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "cbb1a34f",
            "metadata": {},
            "source": [
                "There are still many unrecognized entities because a hardcoded list can never be complete (e.g. \"Ronald\" was not in the original list), and typos can occur. However, this approach may help generate training data."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
