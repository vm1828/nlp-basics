{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "40ce84c3",
            "metadata": {},
            "source": [
                "# SETUP"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "093fa485",
            "metadata": {},
            "source": [
                "## Import"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "7707d486",
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import re\n",
                "import json\n",
                "import random\n",
                "from typing import TypedDict\n",
                "\n",
                "from shared.constants import *\n",
                "from shared.corpus import Corpus\n",
                "from shared.utils import load_json\n",
                "\n",
                "import spacy\n",
                "from spacy.language import Language\n",
                "from spacy.lang.en import English\n",
                "from spacy.training import Example\n",
                "from spacy.util import minibatch\n",
                "\n",
                "# import requests\n",
                "# from bs4 import BeautifulSoup\n",
                "# from itertools import chain\n",
                "# from typing import Any, Iterator"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "0b16abe4",
            "metadata": {},
            "outputs": [],
            "source": [
                "# # Get characters\n",
                "\n",
                "# url = \"https://pemberley.com/janeinfo/ppdrmtis.html\"\n",
                "# res = requests.get(url)\n",
                "# soup = BeautifulSoup(res.content, \"html.parser\")\n",
                "\n",
                "# character_names = []\n",
                "# target_h2 = soup.find(\"h2\", string=\"Brief, Organized Listing of Characters\")\n",
                "# target_ul = target_h2.find_next(\"ul\")\n",
                "# for a in target_ul.find_all(\"a\"):\n",
                "#     name = a.get_text(strip=True).replace('\\n', ' ')\n",
                "#     if name:\n",
                "#         character_names.append(name)\n",
                "\n",
                "# print(character_names)\n",
                "# with open(JSON_CHARACTERS_PNP, \"w\", encoding=\"utf-8\") as f:\n",
                "#     json.dump(character_names, f, indent=4)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8664b4e3",
            "metadata": {},
            "source": [
                "# Rule-Based NER (Gazetteer Method)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "f558c621",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "['Mr. Bennet', 'Mrs. Bennet', 'Jane', 'Elizabeth', 'Mary', 'Kitty', 'Lydia', 'Bingley', 'Louisa Hurst', 'Caroline', 'Mr. Collins', 'Old Mr. Darcy', 'Lady Anne Darcy', 'Darcy', 'Georgiana Darcy', 'Lady Catherine', 'Anne de Bourgh', 'Colonel Fitzwilliam', 'Mr. Gardiner', 'Mrs. Gardiner', 'Sir William', 'Lady Lucas', 'Charlotte', 'Maria', 'Old Mr. Wickham', 'Wickham', 'Mrs. Annesley', 'Captain Carter', 'Mr. Chamberlayne', 'Dawson', 'Mr. Denny', 'Colonel Forster', 'William Goulding', 'Miss Grantley', 'Haggerston', 'The Harringtons', 'Mrs. Hill', 'Mr. Hurst', 'Mrs. Jenkinson', 'Mr. Jones', 'Miss Mary King', 'Mrs. Long', 'Lady Metcalfe', 'Mr. Morris', 'Mrs. Nicholls', 'Mr. Philips', 'Miss Pope', 'Mr. Pratt', 'Mrs. Reynolds', 'Mr. Robinson', 'Mr. Stone', 'Miss Watson', 'The Miss Webbs', 'Mrs. Younge']\n",
                        "['Anne', 'Annesley', 'Bennet', 'Bingley', 'Bourgh', 'Caroline', 'Carter', 'Catherine', 'Chamberlayne', 'Charlotte', 'Collins', 'Darcy', 'Dawson', 'Denny', 'Elizabeth', 'Fitzwilliam', 'Forster', 'Gardiner', 'Georgiana', 'Goulding', 'Grantley', 'Haggerston', 'Harringtons', 'Hill', 'Hurst', 'Jane', 'Jenkinson', 'Jones', 'King', 'Kitty', 'Long', 'Louisa', 'Lucas', 'Lydia', 'Maria', 'Mary', 'Metcalfe', 'Morris', 'Nicholls', 'Philips', 'Pope', 'Pratt', 'Reynolds', 'Robinson', 'Stone', 'Watson', 'Webbs', 'Wickham', 'William', 'Younge']\n"
                    ]
                }
            ],
            "source": [
                "# Load data\n",
                "with open(TXT_PNP, \"r\", encoding=\"utf-8\") as f:\n",
                "    text = f.read().strip()\n",
                "    text = re.split(PATTERN_CHAPTER_LINE, text)[1:6]\n",
                "\n",
                "with open(JSON_CHARACTERS_PNP, \"r\", encoding=\"utf-8\") as f:\n",
                "    characters = json.load(f)\n",
                "    names = set()\n",
                "    for character in characters:\n",
                "        names.update(\n",
                "            [\n",
                "                i for i in character.split() \n",
                "                if not i in ['Mr.', 'The', 'Mrs.', 'Miss', 'Old', 'Lady', 'de', 'Sir', 'Captain', 'Colonel']\n",
                "            ]\n",
                "        )\n",
                "    names = sorted(names)\n",
                "\n",
                "print(characters)\n",
                "print(names)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "c5a3dade",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "\n",
                        "Chapter 1:\n",
                        "Bingley, Jane, Lady Lucas, Lydia, Mr Bennet, Mr Bingley, Mr Morris, Mrs Long, Sir William\n",
                        "\n",
                        "\n",
                        "Chapter 2:\n",
                        "Elizabeth, Kitty, Lydia, Mary, Mr Bennet, Mr Bingley, Mrs Bennet, Mrs Long, Now Kitty, While Mary\n",
                        "\n",
                        "\n",
                        "Chapter 3:\n",
                        "Bingley, Catherine, Come Darcy, Elizabeth, Elizabeth Bennet, Jane, Lady Lucas, Lydia, Maria, Maria Lucas, Mary, Miss Bennet, Miss Bingley, Miss King, Miss Lucas, Mr Bennet, Mr Bingley, Mr Darcy, Mr Hurst, Mrs Bennet, Mrs Hurst, Sir William\n",
                        "\n",
                        "\n",
                        "Chapter 4:\n",
                        "Bingley, Darcy, Elizabeth, Miss Bennet, Miss Bingley, Mr Bingley, Mrs Hurst, When Jane\n",
                        "\n",
                        "\n",
                        "Chapter 5:\n",
                        "Charlotte, Elizabeth, Jane, Lady Lucas, Lucas, Mary, Miss Bennet, Miss Bingley, Miss Lucas, Mr Darcy, Mr Robinson, Mrs Bennet, Mrs Long, Sir William, William Lucas\n"
                    ]
                }
            ],
            "source": [
                "# Characters by Chapter\n",
                "for n_chapter, chapter in enumerate(text, start=1):\n",
                "    print(f'\\n\\nChapter {n_chapter}:')\n",
                "    chapter = chapter.strip().replace('\\n', ' ').translate(str.maketrans('', '', PUNCTUATION))\n",
                "    words = chapter.split() # NOTE: Mr and Mrs will be split\n",
                "\n",
                "    chapter_names = set()\n",
                "    for n, word in enumerate(words):\n",
                "        if word in names:\n",
                "            if (words[n-2] == 'Old') or (words[n-1] == 'de'):\n",
                "                chapter_names.add(' '.join(words[n-2:n+1])) # Handle 3-word names\n",
                "            elif words[n-1][0].isupper():\n",
                "                chapter_names.add(' '.join(words[n-1:n+1])) # Handle 2-word names\n",
                "            else:\n",
                "                chapter_names.add(word)\n",
                "    chapter_names = sorted(chapter_names)\n",
                "    print(', '.join(chapter_names))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8b7c17a2",
            "metadata": {},
            "source": [
                "# NER with SpaCy"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "353dffb6",
            "metadata": {},
            "source": [
                "## Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "e8aa811c",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly norma'"
                        ]
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Data\n",
                "hp_corpus = Corpus(DIR_HP)\n",
                "chapter1 = hp_corpus.books[0].chapters[0].text\n",
                "chapter1[:100]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "c83056b6",
            "metadata": {},
            "outputs": [],
            "source": [
                "# url = \"https://en.wikipedia.org/wiki/List_of_Harry_Potter_characters\"\n",
                "# res = requests.get(url)\n",
                "# soup = BeautifulSoup(res.content, \"html.parser\")\n",
                "\n",
                "# characters = []\n",
                "# divs = soup.find_all(\"div\", class_=\"mw-heading mw-heading3\")\n",
                "# for div in divs:\n",
                "#     ul = div.find_next(\"ul\")\n",
                "#     for li in ul.find_all(\"li\"):\n",
                "#             text = li.get_text(strip=True)\n",
                "#             name = re.search(r\"^(.*?)â€“\", text).group(1).replace(' and ', ',').translate(str.maketrans('()', ',,'))\n",
                "#             if ',' in name:\n",
                "#                 split_name = [part.strip() for part in name.split(',') if part.strip()]\n",
                "#                 last_name = split_name[0]\n",
                "\n",
                "#                 # Case: <last_name>, <name 1>, <name2>, ...\n",
                "#                 if len(split_name[1:]) > 1:\n",
                "#                     for first_name in split_name[1:]:\n",
                "#                         characters.append(f'{first_name.strip()} {last_name.strip()}')\n",
                "#                 # Case: <last_name>, <first_name>\n",
                "#                 else:\n",
                "#                     characters.append(f'{split_name[1].strip()} {last_name.strip()}')\n",
                "#             else:\n",
                "#                 # Case: <name>\n",
                "#                 characters.append(name.strip())\n",
                "\n",
                "# print(characters[:10])\n",
                "# with open(JSON_CHARACTERS_HP, \"w\", encoding=\"utf-8\") as f:\n",
                "#     json.dump(characters, f, indent=4)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "0edc4c26",
            "metadata": {},
            "outputs": [],
            "source": [
                "nlp = spacy.load('en_core_web_lg')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "2f3503be",
            "metadata": {},
            "outputs": [],
            "source": [
                "doc = nlp(chapter1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "1f2cf7b0",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Dursley        PERSON    \n",
                        "number four    CARDINAL  \n",
                        "Privet Drive   PERSON    \n",
                        "Grunnings      ORG       \n",
                        "Dursleys       NORP      \n",
                        "Dudley         PERSON    \n",
                        "Potters        ORG       \n",
                        "Potter         PERSON    \n",
                        "several years  DATE      \n",
                        "Tuesday        DATE      \n"
                    ]
                }
            ],
            "source": [
                "unique_ents = list({ent.text: ent for ent in doc.ents}.values())\n",
                "for ent in unique_ents[:10]:\n",
                "    print(f'{ent.text:<15}{ent.label_:<10}')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5fc7a6f8",
            "metadata": {},
            "source": [
                "We need to customize the model to make NER for the HP corpus better"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "9d337ecf",
            "metadata": {},
            "source": [
                "# MODELS"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "12155170",
            "metadata": {},
            "source": [
                "## Rule-Based Model (with EntityRuler)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b37b446b",
            "metadata": {},
            "source": [
                "Using EntityRuler pipe we can generate rule-based NER with SpaCy.  \n",
                "This approach can be used to generate train datasets."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "6e61a369",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "4360\n",
                        "{'label': 'PERSON', 'pattern': '\"Delphi\"'}\n",
                        "{'label': 'PERSON', 'pattern': 'Abbott'}\n",
                        "{'label': 'PERSON', 'pattern': 'Aberforth'}\n",
                        "{'label': 'PERSON', 'pattern': 'Aberforth Dumbledore'}\n",
                        "{'label': 'PERSON', 'pattern': 'Alastor'}\n"
                    ]
                }
            ],
            "source": [
                "# Generate patterns for characters\n",
                "\n",
                "def generate_characters(characters: list[str]) -> list:\n",
                "    \"\"\"Generate combinations of character names and titles\"\"\"\n",
                "    names = []\n",
                "    titles = [\"Dr.\", \"Professor\", \"Mr.\", \"Mrs.\", \"Ms.\", \"Miss\", \"Aunt\", \"Uncle\", \"Mr. and Mrs.\"]\n",
                "    \n",
                "    for item in characters:\n",
                "        item = item.replace('The', '').replace('the', '').strip()\n",
                "        names.append(item)\n",
                "        for name in item.split():\n",
                "            names.append(name)\n",
                "    \n",
                "    generated_characters = set()\n",
                "    for title in titles:\n",
                "        for name in names:\n",
                "            generated_characters.add(name)\n",
                "            generated_characters.add(f'{title} {name}')\n",
                "    generated_characters = sorted(generated_characters)\n",
                "    return generated_characters\n",
                "\n",
                "def generate_characters_patterns(generated_characters: list) -> list[dict]:\n",
                "    patterns = []\n",
                "    for item in generated_characters:\n",
                "        pattern = {\n",
                "            \"label\": \"PERSON\",\n",
                "            \"pattern\": item\n",
                "        }\n",
                "        patterns.append(pattern)\n",
                "    return (patterns)\n",
                "\n",
                "data: list[str] = load_json(JSON_CHARACTERS_HP)\n",
                "generated_characters = generate_characters(data)\n",
                "patterns = generate_characters_patterns(generated_characters)\n",
                "print(len(patterns))\n",
                "for pattern in patterns[:5]:\n",
                "    print(pattern)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "9a2345a7",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create rule-based NER model with EntityRuler pipe\n",
                "\n",
                "PATH_MODEL_HP_NER_RULE_BASED = os.path.join(DIR_MODELS_SPACY, \"hp_ner_rule_based\")\n",
                "\n",
                "def create_rule_based_model(\n",
                "        patterns: list[dict], \n",
                "        path=PATH_MODEL_HP_NER_RULE_BASED\n",
                "    ) -> None:\n",
                "    \"\"\"Create and save a rule-based NER model using custom patterns.\"\"\"\n",
                "\n",
                "    # Initialize model with an empty pipeline\n",
                "    nlp = English() # no pre-trained components (only basic tokenizer included)\n",
                "\n",
                "    # Add EntityRuler to the model\n",
                "    ruler = nlp.add_pipe(\"entity_ruler\")\n",
                "    ruler.add_patterns(patterns)\n",
                "\n",
                "    # Save model\n",
                "    nlp.to_disk(path)\n",
                "\n",
                "def use_rule_based_model(nlp: Language, text: str) -> list[str]:\n",
                "    \"\"\"Extract recognized entities.\"\"\"\n",
                "    doc = nlp(text)\n",
                "    return sorted(set(ent.text for ent in doc.ents))\n",
                "\n",
                "create_rule_based_model(patterns)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "4443cbbe",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Book 1, Chapter 1\n",
                        "['Albus', 'Albus Dumbledore', 'Dedalus Diggle', 'Dudley', 'Dumbledore', 'Godric', 'Hagrid', 'Harry', 'Harry Potter', 'James', 'James Potter', 'Lily', 'Madam', 'Mr. Dursley', 'Mr. and Mrs. Dursley', 'Mrs. Dursley', 'Mrs. Potter', 'Muggle', 'Petunia', 'Pomfrey', 'Potter', 'Professor Dumbledore', 'Professor McGonagall', 'Sirius', 'Sirius Black', 'Ted', 'Voldemort']\n",
                        "\n",
                        "Book 1, Chapter 2\n",
                        "['Aunt Petunia', 'Dudley', 'Dudley Dursley', 'Harry', 'Harry Potter', 'Marge', 'Mr. Dursley', 'Mrs. Figg', 'Nearly', 'Uncle Vernon', 'Vernon']\n",
                        "\n",
                        "Book 1, Chapter 3\n",
                        "['Aunt Petunia', 'Dudley', 'Harry', 'Marge', 'Mrs. Figg', 'Nearly', 'Petunia', 'Potter', 'Uncle Vernon', 'Vernon']\n"
                    ]
                }
            ],
            "source": [
                "# Test model\n",
                "\n",
                "hp_corpus = Corpus(DIR_HP)\n",
                "nlp = spacy.load(PATH_MODEL_HP_NER_RULE_BASED)\n",
                "\n",
                "for n_book, book in enumerate(hp_corpus.books[:1], start=1):\n",
                "    for n_chapter, chapter in enumerate(book.chapters[:3], start=1): # NOTE: only book 1 chapters 1-3\n",
                "        print(f'\\nBook {n_book}, Chapter {n_chapter}')\n",
                "        results = use_rule_based_model(nlp, chapter.text)\n",
                "        results = sorted(set(results))\n",
                "        print(results)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "cbb1a34f",
            "metadata": {},
            "source": [
                "There are still many unrecognized entities because a hardcoded list can never be complete (e.g. \"Ronald\" was not in the original list), and typos can occur. However, this approach may help generate training data."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5de67b9b",
            "metadata": {},
            "source": [
                "### Generate training data using rule-based NER"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "25a763f5",
            "metadata": {},
            "source": [
                "We generate weakly supervised training data using rule-based NER adn then use this data for training spaCy model. \n",
                "\n",
                "\n",
                "Train data structure:\n",
                "```json\n",
                "[\n",
                "  [\n",
                "    \"<text>\",\n",
                "    {\n",
                "      \"entities\":\n",
                "        [\n",
                "          [<start>, <end>, \"<label>\"], \n",
                "          ...\n",
                "        ]\n",
                "    }\n",
                "  ],\n",
                "  ...\n",
                "]\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "09e5dd56",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Data types for type and field hints\n",
                "TrainDataEntity = tuple[int, int, str]\n",
                "TrainDataItem = tuple[str, dict[str, list[TrainDataEntity]]] # (text, annotation)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "id": "597ec8d6",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "('Mr. and Mrs. Dursley, of numbe', {'entities': [(0, 20, 'PERSON')]})\n",
                        "Mr. and Mrs. Dursley\n"
                    ]
                }
            ],
            "source": [
                "def generate_train_data_item(model: Language, text: str) -> TrainDataItem:\n",
                "    doc = model(text)\n",
                "    entities = []\n",
                "    for ent in doc.ents:\n",
                "        entity: TrainDataEntity = (ent.start_char, ent.end_char, ent.label_)\n",
                "        entities.append(entity)\n",
                "    annotation = {\n",
                "        \"entities\": entities\n",
                "    }\n",
                "    data_item: TrainDataItem = (text, annotation)\n",
                "    return data_item\n",
                "\n",
                "nlp = spacy.load(PATH_MODEL_HP_NER_RULE_BASED)\n",
                "\n",
                "example_entity = generate_train_data_item(nlp, chapter1[:30])\n",
                "print(example_entity)\n",
                "print(chapter1[0:20])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "id": "dd2a0963",
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_train_data(model: Language, text: str):\n",
                "    nlp = spacy.load(\"en_core_web_sm\")\n",
                "\n",
                "    doc = nlp(text)\n",
                "    for sent in doc.sents:\n",
                "        data_item = generate_train_data_item(model, sent.text)\n",
                "        if data_item[1][\"entities\"]:\n",
                "            yield data_item # only data with found entities"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "id": "5a7d462a",
            "metadata": {},
            "outputs": [],
            "source": [
                "# # Generate and save train data\n",
                "\n",
                "# def dump_to_json(generator: Iterator[TrainDataItem], filename: str, chunk_size=1000) -> None:\n",
                "#     with open(filename, 'w', encoding='utf-8') as f:\n",
                "#         f.write('[')\n",
                "#         item = next(generator)\n",
                "#         json.dump(item, f, ensure_ascii=False) # write 1st item\n",
                "\n",
                "#         batch = []\n",
                "#         for item in generator:\n",
                "#             batch.append(item)\n",
                "#             if len(batch) >= chunk_size:\n",
                "#                 f.write(',\\n')\n",
                "#                 items = (json.dumps(item, ensure_ascii=False) for item in batch)\n",
                "#                 f.write(',\\n'.join(items))\n",
                "#                 f.flush()\n",
                "#                 batch.clear()\n",
                "        \n",
                "#         if batch:\n",
                "#             f.write(',\\n')\n",
                "#             items = (json.dumps(batch_item, ensure_ascii=False) for batch_item in batch)\n",
                "#             f.write(',\\n'.join(items))\n",
                "#         f.write('\\n]')\n",
                "\n",
                "# hp_rule_based_ner_model = spacy.load(path_hp_rule_based_ner_model)\n",
                "# en_core_web_lg = spacy.load(\"en_core_web_lg\")\n",
                "\n",
                "# generators = []\n",
                "# for book in hp_corpus.books:\n",
                "#     for chapter in book.chapters:\n",
                "#         generators.append(generate_train_data(hp_rule_based_ner_model, chapter.text))\n",
                "        \n",
                "# dump_to_json(chain.from_iterable(generators), JSON_TRAIN_DATA_HP)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "id": "d2a4d3d4",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "['Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much.', {'entities': [[0, 20, 'PERSON']]}]\n",
                        "['Mr. Dursley was the director of a firm called Grunnings, which made drills.', {'entities': [[0, 11, 'PERSON']]}]\n",
                        "['Mrs. Dursley was thin and blonde and had nearly twice the usual amount of neck, which came in very useful as she spent so much of her time craning over garden fences, spying on the neighbors.', {'entities': [[0, 12, 'PERSON']]}]\n",
                        "43607\n"
                    ]
                }
            ],
            "source": [
                "TRAIN_DATA = load_json(JSON_TRAIN_DATA_HP)\n",
                "for i in TRAIN_DATA[:3]:\n",
                "    print(i)\n",
                "print(len(TRAIN_DATA))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e2bcba82",
            "metadata": {},
            "source": [
                "## Train spaCy Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "id": "6cda80a5",
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_spacy_model(\n",
                "        train_data: list[TrainDataItem], \n",
                "        EPOCHS: int = 30,\n",
                "        BATCH_SIZE: int = 8,\n",
                "        DROPOUT: float = 0.2\n",
                "    ):\n",
                "\n",
                "    # Create empty English pipeline\n",
                "    nlp = spacy.blank(\"en\")\n",
                "\n",
                "    # Create/get NER component\n",
                "    if \"ner\" not in nlp.pipe_names:\n",
                "        ner = nlp.create_pipe(\"ner\")\n",
                "        nlp.add_pipe(\"ner\", last=True)\n",
                "    else:\n",
                "        ner = nlp.get_pipe(\"ner\")\n",
                "    \n",
                "    # Add labels to NER component\n",
                "    for text, annotation in train_data:\n",
                "        for ent in annotation[\"entities\"]:\n",
                "            ner.add_label(ent[2])\n",
                "\n",
                "    # Train NER component\n",
                "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
                "    with nlp.select_pipes(disable=other_pipes): # train only NER component\n",
                "        optimizer = nlp.begin_training()\n",
                "\n",
                "        examples = [\n",
                "            Example.from_dict(\n",
                "                nlp.make_doc(text), \n",
                "                annotation\n",
                "            ) \n",
                "            for text, annotation in train_data\n",
                "        ]\n",
                "        for epoch in range(EPOCHS):\n",
                "            print(\"Epoch\", epoch, end=\" --- \")\n",
                "            random.shuffle(TRAIN_DATA) # ensure that the model doesn't just memorize the order\n",
                "            losses = {}\n",
                "\n",
                "            for batch in minibatch(examples, size=BATCH_SIZE):\n",
                "                nlp.update(batch, sgd=optimizer, drop=DROPOUT, losses=losses)\n",
                "            loss = losses['ner']\n",
                "            print('Loss:', loss)\n",
                "            \n",
                "            if loss < 10: # NOTE: Primitive early stopping \n",
                "                print(f'Stopping early at epoch {epoch}')\n",
                "                break\n",
                "            \n",
                "    return nlp"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "id": "79569576",
            "metadata": {},
            "outputs": [
                {
                    "ename": "SystemExit",
                    "evalue": "Manual execution required",
                    "output_type": "error",
                    "traceback": [
                        "An exception has occurred, use %tb to see the full traceback.\n",
                        "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m Manual execution required\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/asdf/Desktop/nlp-basics/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3587: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
                        "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
                    ]
                }
            ],
            "source": [
                "# Train model\n",
                "\n",
                "# %%capture captured_output\n",
                "sys.exit(\"Manual execution required\");\n",
                "\n",
                "TRAIN_DATA = load_json(JSON_TRAIN_DATA_HP)[:3000]\n",
                "nlp = train_spacy_model(TRAIN_DATA, EPOCHS=30)\n",
                "\n",
                "PATH_MODEL_HP_NER_TRAINED_SIMPLE = os.path.join(DIR_MODELS_SPACY, 'hp_ner_trained_simple')\n",
                "nlp.to_disk(PATH_MODEL_HP_NER_TRAINED_SIMPLE)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "id": "5e43cbaf",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Use Book 1, Chapter 12\n"
                    ]
                }
            ],
            "source": [
                "def find_test_chapter(corpus: Corpus, substr: str) -> tuple[int, int] | None:\n",
                "    for n_book, book in enumerate(corpus.books):\n",
                "        for n_chapter, chapter in enumerate(book.chapters):\n",
                "            if substr in chapter.text:\n",
                "                print(f'Use Book {n_book+1}, Chapter {n_chapter+1}')\n",
                "                return (n_book, n_chapter)\n",
                "            \n",
                "book_idx, chapter_idx = find_test_chapter(hp_corpus, 'Ronald') # type: ignore"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "id": "9f613458",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compare trained and rule-based models\n",
                "\n",
                "hp_corpus = Corpus(DIR_HP)\n",
                "text = hp_corpus.books[book_idx].chapters[chapter_idx].text\n",
                "\n",
                "PATH_MODEL_HP_NER_RULE_BASED = os.path.join(DIR_MODELS_SPACY, 'hp_ner_rule_based')\n",
                "PATH_MODEL_HP_NER_TRAINED_SIMPLE = os.path.join(DIR_MODELS_SPACY, 'hp_ner_trained_simple')\n",
                "hp_model_ner_rule_based = spacy.load(PATH_MODEL_HP_NER_RULE_BASED)\n",
                "hp_model_ner_trained_simple = spacy.load(PATH_MODEL_HP_NER_TRAINED_SIMPLE)\n",
                "results_rule_based = set()\n",
                "results_trained = set()\n",
                "\n",
                "for model, results in [\n",
                "        (hp_model_ner_rule_based, results_rule_based), \n",
                "        (hp_model_ner_trained_simple, results_trained)\n",
                "    ]:\n",
                "    doc = model(text)\n",
                "    unique_names = sorted({ent.text for ent in doc.ents})\n",
                "    for ent in doc.ents:\n",
                "        results.add(ent.text)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "id": "1ccacbaa",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Difference (rule_based - trained): []\n",
                        "Difference (trained - rule_based): ['Forge', 'Quick']\n"
                    ]
                }
            ],
            "source": [
                "print(f\"Difference (rule_based - trained): {sorted(results_rule_based-results_trained)}\")\n",
                "print(f\"Difference (trained - rule_based): {sorted(results_trained-results_rule_based)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "63bf72cd",
            "metadata": {},
            "source": [
                "It still doesn't find 'Ronald'.  \n",
                "The model can be significantly improved with Word Embedding vectors."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
